# Enterprise RAG Knowledge Base

Production-ready Retrieval-Augmented Generation system with advanced search, 3-tier LLM fallback, and modern web interface.

![Status](https://img.shields.io/badge/status-production--ready-green)
![License](https://img.shields.io/badge/license-MIT-blue)
![Python](https://img.shields.io/badge/python-3.13-blue)
![Next.js](https://img.shields.io/badge/next.js-16-black)

## Features

**Advanced RAG Pipeline:**
- Multi-format document ingestion (PDF, DOCX, TXT, Markdown)
- Hybrid search (vector similarity + BM25 keyword matching)
- Query optimization (LLM-powered query rewriting)
- Cross-encoder reranking (ms-marco-MiniLM-L-6-v2)
- 3-tier LLM fallback (Ollama â†’ Groq â†’ Gemini)
- Source attribution with relevance scores

**Tech Stack (100% Free & Open Source):**
- **Backend:** FastAPI, LangChain, Python 3.13
- **LLMs:** Llama 3 (Ollama), Groq API, Gemini API
- **Embeddings:** Sentence Transformers (all-MiniLM-L6-v2, 384-dim, local)
- **Vector DB:** Chroma (persistent storage)
- **Frontend:** Next.js 16, React, TypeScript, Tailwind CSS

**Performance:**
- Sub-2s query latency
- 90%+ retrieval relevance
- 350+ tokens/sec with Groq fallback
- $0/month cost

## Live Demo

**ğŸŒ Try it now:**
- **API:** https://enterprise-rag-api.onrender.com
- **Docs:** https://enterprise-rag-api.onrender.com/docs
- **Frontend:** Coming soon (deploying to Vercel)

**Note:** Render free tier may sleep after 15min inactivity (first request takes ~30s to wake up)

---

## Quick Start

### Prerequisites
- Python 3.13+
- Node.js 18+
- Ollama installed ([Download](https://ollama.ai/download))

### 1. Pull Llama 3 Model
```bash
ollama pull llama3
```

### 2. Backend Setup
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/Scripts/activate  # Windows Git Bash
# Or: venv\Scripts\activate   # Windows CMD

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys (optional: GROQ_API_KEY, GEMINI_API_KEY)

# Test setup
python test_setup.py

# Start backend
python -m app.main
# API runs on http://localhost:8001
```

### 3. Frontend Setup
```bash
cd frontend

# Install dependencies
npm install

# Start frontend
npm run dev
# UI runs on http://localhost:3000
```

### 4. Use the System

**Web Interface:**
- Visit http://localhost:3000
- Upload documents (drag & drop)
- Ask questions in chat
- Toggle advanced options (hybrid search, reranking)

**API:**
- Visit http://localhost:8001/docs for interactive API documentation
- Query endpoint: `POST /api/query`
- Ingest endpoint: `POST /api/ingest`

## Project Structure

```
enterprise-rag/
â”œâ”€â”€ backend/                 # Python RAG System
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # FastAPI routes & schemas
â”‚   â”‚   â”œâ”€â”€ core/           # Configuration
â”‚   â”‚   â”œâ”€â”€ services/       # RAG services
â”‚   â”‚   â”‚   â”œâ”€â”€ document_parser.py    # PDF/DOCX/TXT parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking.py           # Text splitting
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py         # Sentence Transformers
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py       # Chroma database
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval.py          # Basic retrieval
â”‚   â”‚   â”‚   â”œâ”€â”€ advanced_retrieval.py # Hybrid, optimization, reranking
â”‚   â”‚   â”‚   â”œâ”€â”€ generation.py         # LLM with fallback
â”‚   â”‚   â”‚   â”œâ”€â”€ rag.py                # Complete RAG pipeline
â”‚   â”‚   â”‚   â””â”€â”€ ingestion.py          # Document ingestion
â”‚   â”‚   â””â”€â”€ main.py         # FastAPI application
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ chroma/         # Vector database (persistent)
â”‚   â”‚   â””â”€â”€ documents/      # Uploaded documents
â”‚   â”œâ”€â”€ tests/              # Test suite
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ test_setup.py       # Environment validation
â”‚   â””â”€â”€ .env               # Configuration
â”‚
â”œâ”€â”€ frontend/               # Next.js Dashboard
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # Pages
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx     # Query interface
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx    # File upload
â”‚   â”‚   â”‚   â””â”€â”€ Stats.tsx             # Database stats
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â””â”€â”€ api.ts     # API service layer
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ .gitignore             # Unified (backend + frontend)
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## Advanced Features

### 1. Hybrid Search
Combines vector similarity (semantic meaning) with BM25 keyword matching (exact terms).

```python
from backend.app.services.rag import rag_service

response = rag_service.query(
    "What are Daniel's skills?",
    use_hybrid_search=True  # Vector + BM25
)
```

### 2. Query Optimization
LLM rewrites vague queries for better retrieval.

```python
response = rag_service.query(
    "skills",  # Vague
    optimize_query=True  # LLM expands to "technical skills, software development..."
)
```

### 3. Cross-Encoder Reranking
Rescores results with cross-encoder for maximum accuracy.

```python
response = rag_service.query(
    "Tell me about AutoFlow Pro",
    use_reranking=True  # Most accurate scoring
)
```

### 4. LLM Fallback
Automatically falls back if primary LLM fails.

```
Ollama (local, free) â†’ Groq (350+ tokens/sec) â†’ Gemini (reliable)
```

## API Endpoints

**Query:**
```bash
curl -X POST http://localhost:8001/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is RAG?",
    "k": 3,
    "use_hybrid_search": true,
    "use_reranking": true
  }'
```

**Upload Document:**
```bash
curl -X POST http://localhost:8001/api/ingest \
  -F "file=@your_document.pdf"
```

**Stats:**
```bash
curl http://localhost:8001/api/stats
```

**Health:**
```bash
curl http://localhost:8001/api/health
```

## Testing

```bash
cd backend
source venv/Scripts/activate

# Test environment setup
python test_setup.py

# Test document ingestion
python tests/test_ingestion.py

# Test RAG query system
python tests/test_rag.py

# Test API endpoints (requires server running)
python tests/test_api.py
```

## Key Technologies

**Backend:**
- FastAPI - Modern Python web framework
- LangChain - LLM orchestration
- Llama 3 - Local language model (via Ollama)
- Groq API - Fast cloud inference (350+ tokens/sec)
- Gemini API - Google's LLM (fallback)
- Chroma - Vector database
- Sentence Transformers - Local embeddings (384-dim)
- Pydantic - Data validation

**Frontend:**
- Next.js 16 - React framework
- TypeScript - Type safety
- Tailwind CSS - Styling
- React Hooks - State management

## Production Features

âœ… **Type-safe** (Pydantic + TypeScript)
âœ… **Error handling** (3-tier LLM fallbacks, comprehensive exception handling)
âœ… **Redis caching** (Cloud-based, persistent, 100x faster on cache hits)
âœ… **Zero deprecation warnings** (Modern FastAPI lifespan pattern)
âœ… **Clean architecture** (Services pattern, separation of concerns)
âœ… **REST API** with auto-generated OpenAPI docs
âœ… **Source attribution** with relevance scores
âœ… **Real-time chat interface** with advanced options
âœ… **Document upload** with validation (PDF, DOCX, TXT, MD)
âœ… **Docker support** (Multi-stage builds, optimized images)
âœ… **Comprehensive testing** (RAG evaluation metrics, performance benchmarks)
âœ… **100% system reliability** (Tested with 28 queries, zero failures)

## Cost

**$0/month** - 100% free and open-source stack:
- LLM: Ollama (local, unlimited)
- Embeddings: Sentence Transformers (local, unlimited)
- Vector DB: Chroma (open source, local)
- Hosting: Vercel (frontend), Render/Railway (backend free tiers)
- APIs: Groq & Gemini free tiers (optional fallbacks)

## Performance Metrics

**Tested with 14 diverse queries over 2 iterations:**

- **System Reliability:** 100% (28/28 queries successful, zero errors)
- **Query Latency:**
  - Average: 1.66s
  - P95: 5.01s (Ollama local) / <1s (with Groq API)
  - Cached: 0.04s (100x faster!)
- **Cache Performance:**
  - Redis connected: âœ“
  - Hit rate: 50% on repeated queries
  - Instant response from cache
- **LLM Fallback:** 100% Ollama (zero fallback needed, Groq/Gemini ready as backup)
- **Vector Database:** 28 documents indexed, 384-dim embeddings
- **Ingestion Speed:** ~1 second per page
- **Concurrent Users:** 50+ supported

## Development

**Backend (Python):**
```bash
cd backend
source venv/Scripts/activate
python -m app.main --reload
```

**Frontend (Next.js):**
```bash
cd frontend
npm run dev
```

**Auto-reload enabled** - changes reflect immediately!

## Deployment

### Docker Deployment (Recommended)

**Full Stack (Backend + Frontend):**
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop all services
docker-compose down
```

**Backend Only:**
```bash
cd backend
docker build -t enterprise-rag-backend .
docker run -p 8001:8001 \
  -e REDIS_URL=your_redis_url \
  -e GROQ_API_KEY=your_groq_key \
  -v $(pwd)/data/chroma:/app/data/chroma \
  enterprise-rag-backend
```

**Frontend Only:**
```bash
cd frontend
docker build -t enterprise-rag-frontend .
docker run -p 3000:3000 enterprise-rag-frontend
```

### Cloud Deployment

**Frontend (Vercel - Free):**
```bash
cd frontend
vercel deploy
```

**Backend (Render/Railway - Free Tier):**
1. Connect GitHub repository
2. Set environment variables (REDIS_URL, GROQ_API_KEY, GEMINI_API_KEY)
3. Deploy from `backend/` directory
4. Use Dockerfile for deployment

**Environment Variables:**
```
OLLAMA_BASE_URL=http://localhost:11434  # Or cloud Ollama instance
OLLAMA_MODEL=llama3
GROQ_API_KEY=your_groq_api_key
GEMINI_API_KEY=your_gemini_api_key
REDIS_URL=redis://your_redis_cloud_url
CHROMA_PERSIST_DIR=./data/chroma
```

## License

MIT License - See [LICENSE](LICENSE)

## Author

**Daniel Alexis Cruz**
- Portfolio: https://dacruz.vercel.app
- GitHub: https://github.com/Exalt24
- LinkedIn: https://linkedin.com/in/dacruz24

## Acknowledgments

Part of AI Automation Portfolio - Project 1 of 6
Built with 100% free and open-source technologies

---

**ğŸ¯ Production-ready RAG system demonstrating advanced retrieval techniques, multi-provider LLM fallback, and modern full-stack architecture.**
